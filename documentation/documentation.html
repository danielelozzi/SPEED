<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speed Analyzer Documentation</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        /* Custom scrollbar for better aesthetics */
        ::-webkit-scrollbar {
            width: 8px;
        }
        ::-webkit-scrollbar-track {
            background: #f1f1f1;
        }
        ::-webkit-scrollbar-thumb {
            background: #888;
            border-radius: 4px;
        }
        ::-webkit-scrollbar-thumb:hover {
            background: #555;
        }
        /* Styles for code blocks */
        pre code {
            display: block;
            background: #2d3748;
            color: #e2e8f0;
            padding: 1rem;
            border-radius: 0.5rem;
            overflow-x: auto;
            font-family: 'Courier New', Courier, monospace;
        }
        code {
            background-color: #e2e8f0;
            color: #2d3748;
            padding: 2px 5px;
            border-radius: 4px;
            font-size: 0.9em;
        }
        /* Active state for navigation */
        .nav-link.active {
            background-color: #4f46e5;
            color: white;
            font-weight: 600;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <div class="flex">
        <!-- Sidebar Navigation -->
        <aside class="w-64 h-screen fixed top-0 left-0 bg-white border-r border-gray-200 p-5 overflow-y-auto">
            <h1 class="text-xl font-bold text-gray-900 mb-6">Speed Analyzer</h1>
            <nav>
                <ul class="space-y-2" id="nav-menu">
                    <li><a href="#introduction" class="nav-link block px-4 py-2 text-gray-600 hover:bg-indigo-50 hover:text-indigo-600 rounded-md transition-colors duration-200">Introduction</a></li>
                    <li><a href="#bids-converter" class="nav-link block px-4 py-2 text-gray-600 hover:bg-indigo-50 hover:text-indigo-600 rounded-md transition-colors duration-200">BIDS Converter</a></li>
                    <li><a href="#dicom-converter" class="nav-link block px-4 py-2 text-gray-600 hover:bg-indigo-50 hover:text-indigo-600 rounded-md transition-colors duration-200">DICOM Converter</a></li>
                    <li><a href="#device-converters" class="nav-link block px-4 py-2 text-gray-600 hover:bg-indigo-50 hover:text-indigo-600 rounded-md transition-colors duration-200">Device Converters</a></li>
                    <li><a href="#realtime-analyzer" class="nav-link block px-4 py-2 text-gray-600 hover:bg-indigo-50 hover:text-indigo-600 rounded-md transition-colors duration-200">Real-time Analyzer</a></li>
                    <li><a href="#event-analysis" class="nav-link block px-4 py-2 text-gray-600 hover:bg-indigo-50 hover:text-indigo-600 rounded-md transition-colors duration-200">Event-based Analysis</a></li>
                    <li><a href="#video-generator" class="nav-link block px-4 py-2 text-gray-600 hover:bg-indigo-50 hover:text-indigo-600 rounded-md transition-colors duration-200">Video Generator</a></li>
                    <li><a href="#yolo-analyzer" class="nav-link block px-4 py-2 text-gray-600 hover:bg-indigo-50 hover:text-indigo-600 rounded-md transition-colors duration-200">YOLO Analyzer</a></li>
                    <li><a href="#interactive-editor" class="nav-link block px-4 py-2 text-gray-600 hover:bg-indigo-50 hover:text-indigo-600 rounded-md transition-colors duration-200">Interactive Editor</a></li>
                </ul>
            </nav>
        </aside>

        <!-- Main Content -->
        <main class="ml-64 p-10 w-full">
            
            <!-- Introduction Section -->
            <section id="introduction" class="mb-16 scroll-mt-20">
                <h1 class="text-4xl font-bold text-gray-900 border-b pb-4 mb-6">Speed Analyzer Documentation</h1>
                <p class="text-lg text-gray-600 mb-6">
                    Welcome to the Speed Analyzer library documentation. This library is a comprehensive toolkit for processing, analyzing, visualizing, and converting eye-tracking data. It includes modules for real-time analysis, video generation with data overlays, object detection correlation, and conversion to standard scientific formats like BIDS and DICOM.
                </p>
                <h2 class="text-3xl font-semibold text-gray-800 mt-10 mb-4">Modules</h2>
                <p class="text-gray-600 mb-6">
                    Below is an overview of the different modules available in this library. Each module is designed to handle a specific part of the eye-tracking data analysis workflow.
                </p>
                <!-- Module Cards -->
                <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
                    <div class="bg-white p-6 rounded-lg border border-gray-200 shadow-sm">
                        <h3 class="font-bold text-xl mb-2 text-indigo-600">BIDS Converter</h3>
                        <p class="text-gray-600">Handles the conversion of raw eye-tracking data to the Brain Imaging Data Structure (BIDS) format and vice-versa.</p>
                    </div>
                    <div class="bg-white p-6 rounded-lg border border-gray-200 shadow-sm">
                        <h3 class="font-bold text-xl mb-2 text-indigo-600">DICOM Converter</h3>
                        <p class="text-gray-600">Manages the conversion of eye-tracking data into a DICOM Waveform object for integration into medical imaging workflows.</p>
                    </div>
                    <div class="bg-white p-6 rounded-lg border border-gray-200 shadow-sm">
                        <h3 class="font-bold text-xl mb-2 text-indigo-600">Device Converters</h3>
                        <p class="text-gray-600">A high-level module for converting data from specific commercial eye-tracking devices (e.g., Tobii) into standard formats.</p>
                    </div>
                    <div class="bg-white p-6 rounded-lg border border-gray-200 shadow-sm">
                        <h3 class="font-bold text-xl mb-2 text-indigo-600">Real-time Analyzer</h3>
                        <p class="text-gray-600">Connect to a Pupil Labs Neon device, stream data in real-time, perform live object detection, and visualize data on the live video feed.</p>
                    </div>
                     <div class="bg-white p-6 rounded-lg border border-gray-200 shadow-sm">
                        <h3 class="font-bold text-xl mb-2 text-indigo-600">Event-based Analysis</h3>
                        <p class="text-gray-600">Segments data based on events, calculates summary statistics, and generates various scientific plots on demand.</p>
                    </div>
                    <div class="bg-white p-6 rounded-lg border border-gray-200 shadow-sm">
                        <h3 class="font-bold text-xl mb-2 text-indigo-600">Video Generator</h3>
                        <p class="text-gray-600">Creates custom videos by overlaying synchronized eye-tracking data and object detection results onto the original scene video.</p>
                    </div>
                     <div class="bg-white p-6 rounded-lg border border-gray-200 shadow-sm">
                        <h3 class="font-bold text-xl mb-2 text-indigo-600">YOLO Analyzer</h3>
                        <p class="text-gray-600">Applies YOLO models to detect objects and correlates this information with eye-tracking data to generate statistics.</p>
                    </div>
                    <div class="bg-white p-6 rounded-lg border border-gray-200 shadow-sm">
                        <h3 class="font-bold text-xl mb-2 text-indigo-600">Interactive Video Editor</h3>
                        <p class="text-gray-600">A powerful GUI tool for adding/editing events, running multi-task YOLO analysis, and exporting videos with custom overlays.</p>
                    </div>
                </div>
            </section>

            <!-- BIDS Converter Section -->
            <section id="bids-converter" class="mb-16 scroll-mt-20">
                <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">BIDS Converter (<code>bids_converter.py</code>)</h2>
                <p class="text-gray-600 mb-8">This module provides functions to convert eye-tracking data to and from the <strong>Brain Imaging Data Structure (BIDS)</strong> format. BIDS is a standard for organizing and describing neuroimaging and psychological data, which makes datasets easier to share and use.</p>
                
                <div class="bg-white p-6 rounded-lg border border-gray-200">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3"><code>convert_to_bids(...)</code></h3>
                    <p class="text-gray-600 mb-4">Converts raw eye-tracking data from a standard directory structure into the BIDS format. It creates the necessary folder hierarchy and generates BIDS-compliant <code>.tsv.gz</code> and <code>.json</code> files.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Parameters:</h4>
                    <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li><code>unenriched_dir</code> (Path): The path to the directory containing the raw eye-tracking CSV files.</li>
                        <li><code>output_bids_dir</code> (Path): The root directory where the BIDS dataset will be created.</li>
                        <li><code>subject_id</code> (str): The subject identifier (e.g., "01").</li>
                        <li><code>session_id</code> (str): The session identifier (e.g., "01").</li>
                        <li><code>task_name</code> (str): The name of the task performed during the recording.</li>
                    </ul>
                </div>

                <div class="bg-white p-6 rounded-lg border border-gray-200 mt-6">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3"><code>load_from_bids(...)</code></h3>
                    <p class="text-gray-600 mb-4">Loads an eye-tracking dataset from a BIDS-compliant directory and converts it back into the raw format expected by other modules in this library.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Parameters:</h4>
                     <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li><code>bids_dir</code> (Path): The root directory of the BIDS dataset.</li>
                        <li><code>subject_id</code> (str): The subject identifier to load.</li>
                        <li><code>session_id</code> (str): The session identifier to load.</li>
                        <li><code>task_name</code> (str): The task name to load.</li>
                    </ul>
                     <h4 class="font-semibold text-lg mt-4 mb-2">Returns:</h4>
                     <p class="text-gray-600">A <code>Path</code> object pointing to a temporary directory containing the converted, "un-enriched" data files.</p>
                </div>
            </section>
            
            <!-- DICOM Converter Section -->
            <section id="dicom-converter" class="mb-16 scroll-mt-20">
                <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">DICOM Converter (<code>dicom_converter.py</code>)</h2>
                <p class="text-gray-600 mb-8">This module handles the conversion of eye-tracking data into and out of the <strong>DICOM (Digital Imaging and Communications in Medicine)</strong> format. It uses the <strong>Waveform IOD</strong> to store time-series data like gaze position and pupil diameter within a standard DICOM file.</p>
                
                <div class="bg-white p-6 rounded-lg border border-gray-200">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3"><code>convert_to_dicom(...)</code></h3>
                    <p class="text-gray-600 mb-4">Reads raw eye-tracking data from CSV files and packages it into a single DICOM file.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Parameters:</h4>
                    <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li><code>unenriched_dir</code> (Path): The directory containing the raw CSV files.</li>
                        <li><code>output_dicom_path</code> (Path): The full path where the output <code>.dcm</code> file will be saved.</li>
                        <li><code>patient_info</code> (dict): A dictionary containing patient information, such as <code>name</code> and <code>id</code>.</li>
                    </ul>
                </div>

                <div class="bg-white p-6 rounded-lg border border-gray-200 mt-6">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3"><code>load_from_dicom(...)</code></h3>
                    <p class="text-gray-600 mb-4">Extracts eye-tracking data from a DICOM file and reconstructs the original raw data CSV files in a temporary directory.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Parameters:</h4>
                     <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li><code>dicom_path</code> (Path): The path to the input DICOM file.</li>
                    </ul>
                     <h4 class="font-semibold text-lg mt-4 mb-2">Returns:</h4>
                     <p class="text-gray-600">A <code>Path</code> object pointing to a temporary directory containing the reconstructed data files.</p>
                </div>
            </section>

            <!-- Device Converters Section -->
             <section id="device-converters" class="mb-16 scroll-mt-20">
                <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">Device Converters (<code>device_converters.py</code>)</h2>
                <p class="text-gray-600 mb-8">This module is designed to act as a high-level interface for converting data from specific, proprietary eye-tracker formats (like those from Tobii) into standardized formats like BIDS.</p>
                
                <div class="bg-white p-6 rounded-lg border border-gray-200">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3"><code>convert_device_data(...)</code></h3>
                    <p class="text-gray-600 mb-4">Acts as a dispatcher or router function. It takes the name of a device and the desired output format, then calls the appropriate conversion function.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Parameters:</h4>
                    <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li><code>device_name</code> (str): The name of the source device (e.g., "tobii").</li>
                        <li><code>source_folder</code> (str): The path to the folder containing the raw data from the device.</li>
                        <li><code>output_folder</code> (str): The path where the converted data will be saved.</li>
                        <li><code>output_format</code> (str): The desired output format (e.g., "bids").</li>
                        <li><code>bids_info</code> (Optional[Dict]): Required dictionary for BIDS conversion.</li>
                    </ul>
                </div>
            </section>

            <!-- Real-time Analyzer Section -->
            <section id="realtime-analyzer" class="mb-16 scroll-mt-20">
                <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">Real-time Analyzer (<code>realtime_analyzer.py</code>)</h2>
                <p class="text-gray-600 mb-8">This module provides the <code>RealtimeNeonAnalyzer</code> class, a powerful tool for connecting to a <strong>Pupil Labs Neon</strong> eye-tracking device, streaming data in real-time, and performing live analysis and visualization.</p>
                
                <div class="bg-white p-6 rounded-lg border border-gray-200">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3">Class: <code>RealtimeNeonAnalyzer</code></h3>
                    <p class="text-gray-600 mb-4">Manages the entire real-time analysis workflow, from device connection to data visualization and recording.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Key Methods:</h4>
                    <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li><code>__init__(...)</code>: Initializes the analyzer, loading the specified YOLO model.</li>
                        <li><code>connect(...)</code>: Searches for and connects to a Neon device on the local network.</li>
                        <li><code>start_recording(...)</code>: Starts recording raw video streams and eye-tracking data.</li>
                        <li><code>stop_recording()</code>: Stops the recording and finalizes all data files.</li>
                        <li><code>add_event(...)</code>: Adds a timestamped event marker to the recording.</li>
                        <li><code>add_static_aoi(...)</code>: Defines a static Area of Interest for live analysis.</li>
                        <li><code>process_and_visualize(...)</code>: Fetches the latest data, runs YOLO, and draws all enabled overlays onto the scene video frame.</li>
                        <li><code>close()</code>: Safely disconnects from the device.</li>
                    </ul>
                    <h4 class="font-semibold text-lg mt-4 mb-2">New Features:</h4>
                    <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li>**Audio Recording**: Can now record audio alongside video streams.</li>
                        <li>**Advanced Visualizations**: Includes a dynamic heatmap, gaze path with fading trail, and a dedicated blink detection plot.</li>
                    </ul>
                </div>
            </section>

             <!-- Event-based Analysis Section -->
            <section id="event-analysis" class="mb-16 scroll-mt-20">
                <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">Event-based Analysis (<code>speed_script_events.py</code>)</h2>
                <p class="text-gray-600 mb-8">A powerful analysis script to segment eye-tracking data based on recorded events, compute a comprehensive set of metrics for each segment, and generate a variety of scientific plots on demand.</p>
                
                <div class="bg-white p-6 rounded-lg border border-gray-200">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3"><code>run_analysis(...)</code></h3>
                    <p class="text-gray-600 mb-4">The main function for the analysis stage. It loads data, segments it by event, calculates summary statistics, and saves the processed data for later use.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Functionality:</h4>
                    <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li>Loads all relevant CSV files (events, gaze, fixations, etc.).</li>
                        <li>Segments data based on event timestamps.</li>
                        <li>Calculates features like fixation count, duration, pupil diameter, and gaze speed for each segment.</li>
                        <li>Saves processed data per segment to <code>.pkl</code> files for efficient plotting.</li>
                        <li>Saves an aggregated summary of all features to a main CSV file.</li>
                    </ul>
                </div>
                <div class="bg-white p-6 rounded-lg border border-gray-200 mt-6">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3"><code>generate_plots_on_demand(...)</code></h3>
                    <p class="text-gray-600 mb-4">Generates visualizations based on the pre-processed data. It uses multiprocessing to efficiently create plots for each segment.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Generated Plots:</h4>
                    <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li>Histograms of fixation, blink, and saccade durations.</li>
                        <li>Scanpath plots and heatmaps for gaze and fixations.</li>
                        <li>Time-series plots for pupillometry and gaze fragmentation.</li>
                        <li>Spectral analysis (periodogram and spectrogram) for pupil data.</li>
                    </ul>
                </div>
            </section>

            <!-- Video Generator Section -->
            <section id="video-generator" class="mb-16 scroll-mt-20">
                <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">Video Generator (<code>video_generator.py</code>)</h2>
                <p class="text-gray-600 mb-8">This module is dedicated to creating high-quality videos that visualize eye-tracking data by overlaying it onto the original scene recording. It's highly customizable, allowing the user to select which data streams and visualizations to include.</p>
                
                <div class="bg-white p-6 rounded-lg border border-gray-200">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3"><code>create_custom_video(...)</code></h3>
                    <p class="text-gray-600 mb-4">This is the main function that orchestrates the entire video generation process. It reads the source video and eye-tracking data, iterates through each frame, draws the selected overlays, and encodes the final video file.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Parameters:</h4>
                    <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li><code>data_dir</code> (Path): Directory containing synchronized data and the scene video.</li>
                        <li><code>output_dir</code> (Path): Directory where the final video will be saved.</li>
                        <li><code>subj_name</code> (str): The subject identifier.</li>
                        <li><code>options</code> (dict): A dictionary of boolean flags to control overlays (e.g., <code>overlay_gaze</code>, <code>overlay_pupil_plot</code>, <code>overlay_dynamic_heatmap</code>).</li>
                        <li><code>un_enriched_mode</code> (bool): If false, looks for enriched data to add more context.</li>
                        <li><code>selected_events</code> (list, optional): A list of event names to trim the video to.</li>
                    </ul>
                </div>
            </section>

            <!-- YOLO Analyzer Section -->
            <section id="yolo-analyzer" class="mb-16 scroll-mt-20">
                <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">YOLO Analyzer (<code>yolo_analyzer.py</code>)</h2>
                <p class="text-gray-600 mb-8">This module integrates <strong>YOLO (You Only Look Once)</strong> object detection models into the eye-tracking analysis workflow. It runs a YOLO model on the scene video, tracks objects, and correlates this with eye movements to understand what was being looked at.</p>
                
                <div class="bg-white p-6 rounded-lg border border-gray-200">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3"><code>run_yolo_analysis(...)</code></h3>
                    <p class="text-gray-600 mb-4">The main function of the module. It orchestrates the entire process from loading data to running the model, correlating results, and saving statistical outputs.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Parameters:</h4>
                    <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li><code>data_dir</code> (Path): Path to the directory containing eye-tracking data and the scene video.</li>
                        <li><code>output_dir</code> (Path): Path where analysis results will be saved.</li>
                        <li><code>yolo_models</code> (Dict[str, str]): A dictionary mapping a task (e.g., 'detect', 'segment') to a model file name (e.g., 'yolov8n.pt').</li>
                        <li><code>custom_classes</code> (Optional[List[str]]): A list of class names for zero-shot detection.</li>
                        <li><code>yolo_detections_df</code> (Optional[pd.DataFrame]): A pre-filtered DataFrame of YOLO detections to use, bypassing video analysis.</li>
                    </ul>
                     <h4 class="font-semibold text-lg mt-4 mb-2">Functionality:</h4>
                     <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li>**Multi-Task Analysis**: Can run detection, segmentation, and pose estimation models simultaneously on the video.</li>
                        <li>Runs YOLO tracking on the video or loads from a cache file.</li>
                        <li>Correlates fixations with detected object bounding boxes (or segmentation masks).</li>
                        <li>Calculates statistics like fixation count per object and average pupil size.</li>
                        <li>Computes a <strong>Normalized Switching Index (SI)</strong> to quantify attention shifts between objects.</li>
                        <li>Saves all results to CSV files.</li>
                    </ul>
                </div>
            </section>

            <!-- Interactive Video Editor Section -->
            <section id="interactive-editor" class="mb-16 scroll-mt-20">
                <h2 class="text-3xl font-bold text-gray-900 border-b pb-4 mb-6">Interactive Video Editor (<code>interactive_video_editor.py</code>)</h2>
                <p class="text-gray-600 mb-8">This module provides a comprehensive graphical interface for advanced, interactive analysis of a recorded session. It combines event management with powerful, on-the-fly computer vision analysis.</p>
                
                <div class="bg-white p-6 rounded-lg border border-gray-200">
                    <h3 class="text-2xl font-semibold text-gray-800 mb-3">Class: <code>InteractiveVideoEditor</code></h3>
                    <p class="text-gray-600 mb-4">A Toplevel Tkinter window that allows users to visualize data, manage events, and run complex analyses directly on a video.</p>
                    <h4 class="font-semibold text-lg mt-4 mb-2">Key Features:</h4>
                    <ul class="list-disc list-inside space-y-2 text-gray-600">
                        <li><strong>Event Management</strong>: Add, remove, and drag-and-drop event markers directly on an interactive timeline.</li>
                        <li><strong>Multi-Task YOLO Analysis</strong>: Select and run different YOLO models for object detection, segmentation, and pose estimation simultaneously. The results from all tasks are merged and visualized.</li>
                        <li><strong>Interactive Filtering</strong>: Use a tabbed interface to view detected objects, segmented areas, and poses. Users can toggle the visibility of entire classes or individual tracked instances.</li>
                        <li><strong>Audio Playback</strong>: Plays the video with its original audio, which can be muted or unmuted.</li>
                        <li><strong>Overlay Control</strong>: All YOLO results (bounding boxes, segmentation masks, and pose skeletons) are overlaid on the video, respecting the user's filter selections.</li>
                        <li><strong>Video Export</strong>: Export the current view as a new video file, with all selected overlays and optional audio baked in.</li>
                        <li><strong>Data Persistence</strong>: When saving, the editor returns both the modified event data and a DataFrame containing only the YOLO detections that were visible (not filtered out) by the user.</li>
                    </ul>
                </div>
            </section>

        </main>
    </div>

    <script>
        // Simple script for active navigation link highlighting
        document.addEventListener('DOMContentLoaded', () => {
            const sections = document.querySelectorAll('section');
            const navLinks = document.querySelectorAll('.nav-link');

            const observer = new IntersectionObserver((entries) => {
                entries.forEach(entry => {
                    if (entry.isIntersecting) {
                        navLinks.forEach(link => {
                            link.classList.remove('active');
                            if (link.getAttribute('href').substring(1) === entry.target.id) {
                                link.classList.add('active');
                            }
                        });
                    }
                });
            }, { threshold: 0.5 }); // Trigger when 50% of the section is visible

            sections.forEach(section => {
                observer.observe(section);
            });
        });
    </script>
</body>
</html>
